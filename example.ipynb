{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fixed acidity</th>\n      <th>volatile acidity</th>\n      <th>citric acid</th>\n      <th>residual sugar</th>\n      <th>chlorides</th>\n      <th>free sulfur dioxide</th>\n      <th>total sulfur dioxide</th>\n      <th>density</th>\n      <th>pH</th>\n      <th>sulphates</th>\n      <th>alcohol</th>\n      <th>quality</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7.4</td>\n      <td>0.700</td>\n      <td>0.00</td>\n      <td>1.9</td>\n      <td>0.076</td>\n      <td>11.0</td>\n      <td>34.0</td>\n      <td>0.99780</td>\n      <td>3.51</td>\n      <td>0.56</td>\n      <td>9.4</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7.8</td>\n      <td>0.880</td>\n      <td>0.00</td>\n      <td>2.6</td>\n      <td>0.098</td>\n      <td>25.0</td>\n      <td>67.0</td>\n      <td>0.99680</td>\n      <td>3.20</td>\n      <td>0.68</td>\n      <td>9.8</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7.8</td>\n      <td>0.760</td>\n      <td>0.04</td>\n      <td>2.3</td>\n      <td>0.092</td>\n      <td>15.0</td>\n      <td>54.0</td>\n      <td>0.99700</td>\n      <td>3.26</td>\n      <td>0.65</td>\n      <td>9.8</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11.2</td>\n      <td>0.280</td>\n      <td>0.56</td>\n      <td>1.9</td>\n      <td>0.075</td>\n      <td>17.0</td>\n      <td>60.0</td>\n      <td>0.99800</td>\n      <td>3.16</td>\n      <td>0.58</td>\n      <td>9.8</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7.4</td>\n      <td>0.700</td>\n      <td>0.00</td>\n      <td>1.9</td>\n      <td>0.076</td>\n      <td>11.0</td>\n      <td>34.0</td>\n      <td>0.99780</td>\n      <td>3.51</td>\n      <td>0.56</td>\n      <td>9.4</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1594</th>\n      <td>6.2</td>\n      <td>0.600</td>\n      <td>0.08</td>\n      <td>2.0</td>\n      <td>0.090</td>\n      <td>32.0</td>\n      <td>44.0</td>\n      <td>0.99490</td>\n      <td>3.45</td>\n      <td>0.58</td>\n      <td>10.5</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1595</th>\n      <td>5.9</td>\n      <td>0.550</td>\n      <td>0.10</td>\n      <td>2.2</td>\n      <td>0.062</td>\n      <td>39.0</td>\n      <td>51.0</td>\n      <td>0.99512</td>\n      <td>3.52</td>\n      <td>0.76</td>\n      <td>11.2</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>1596</th>\n      <td>6.3</td>\n      <td>0.510</td>\n      <td>0.13</td>\n      <td>2.3</td>\n      <td>0.076</td>\n      <td>29.0</td>\n      <td>40.0</td>\n      <td>0.99574</td>\n      <td>3.42</td>\n      <td>0.75</td>\n      <td>11.0</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>1597</th>\n      <td>5.9</td>\n      <td>0.645</td>\n      <td>0.12</td>\n      <td>2.0</td>\n      <td>0.075</td>\n      <td>32.0</td>\n      <td>44.0</td>\n      <td>0.99547</td>\n      <td>3.57</td>\n      <td>0.71</td>\n      <td>10.2</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1598</th>\n      <td>6.0</td>\n      <td>0.310</td>\n      <td>0.47</td>\n      <td>3.6</td>\n      <td>0.067</td>\n      <td>18.0</td>\n      <td>42.0</td>\n      <td>0.99549</td>\n      <td>3.39</td>\n      <td>0.66</td>\n      <td>11.0</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n<p>1599 rows Ã— 12 columns</p>\n</div>",
      "text/plain": "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n0               7.4             0.700         0.00             1.9      0.076   \n1               7.8             0.880         0.00             2.6      0.098   \n2               7.8             0.760         0.04             2.3      0.092   \n3              11.2             0.280         0.56             1.9      0.075   \n4               7.4             0.700         0.00             1.9      0.076   \n...             ...               ...          ...             ...        ...   \n1594            6.2             0.600         0.08             2.0      0.090   \n1595            5.9             0.550         0.10             2.2      0.062   \n1596            6.3             0.510         0.13             2.3      0.076   \n1597            5.9             0.645         0.12             2.0      0.075   \n1598            6.0             0.310         0.47             3.6      0.067   \n\n      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n0                    11.0                  34.0  0.99780  3.51       0.56   \n1                    25.0                  67.0  0.99680  3.20       0.68   \n2                    15.0                  54.0  0.99700  3.26       0.65   \n3                    17.0                  60.0  0.99800  3.16       0.58   \n4                    11.0                  34.0  0.99780  3.51       0.56   \n...                   ...                   ...      ...   ...        ...   \n1594                 32.0                  44.0  0.99490  3.45       0.58   \n1595                 39.0                  51.0  0.99512  3.52       0.76   \n1596                 29.0                  40.0  0.99574  3.42       0.75   \n1597                 32.0                  44.0  0.99547  3.57       0.71   \n1598                 18.0                  42.0  0.99549  3.39       0.66   \n\n      alcohol  quality  \n0         9.4        5  \n1         9.8        5  \n2         9.8        5  \n3         9.8        6  \n4         9.4        5  \n...       ...      ...  \n1594     10.5        5  \n1595     11.2        6  \n1596     11.0        6  \n1597     10.2        5  \n1598     11.0        6  \n\n[1599 rows x 12 columns]"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader,WeightedRandomSampler\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler    \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "df = pd.read_csv('Data/winequality-red.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<AxesSubplot:xlabel='quality', ylabel='count'>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAATtUlEQVR4nO3df7CmZX3f8fdHFvxBlQU53eIudplmB4dpK8IZiiEx1i0pEGWpQyhOhS2ls7aDjtq0KWlmmjSTzJipqQFt6WwhuBiVIJGwOtTKrL+iLcSzgIBgykpAdruwJ8oPkRKLfvvHc52bh+UA58C5n3v37Ps1c89z3dd93c/zfWZn97PX/etJVSFJEsDLhi5AkrTvMBQkSR1DQZLUMRQkSR1DQZLUWTF0AS/FkUceWWvXrh26DEnar2zfvv0vq2pqvm37dSisXbuWmZmZocuQpP1Kkvufa1tvh4+SHJvktrHlsSQfSHJEkhuT3NNeD2/jk+TSJDuS3J7khL5qkyTNr7dQqKo/r6rjq+p44ETgCeA64GJgW1WtA7a1dYDTgXVt2QRc1ldtkqT5TepE83rgu1V1P7AB2NL6twBntfYG4KoauQlYmeSoCdUnSWJyoXAu8OnWXlVVu1v7QWBVa68GHhjbZ2fre4Ykm5LMJJmZnZ3tq15JOiD1HgpJDgHOBD6z97YaPXhpUQ9fqqrNVTVdVdNTU/OePJckvUiTmCmcDtxSVQ+19YfmDgu11z2tfxdw9Nh+a1qfJGlCJhEK7+LpQ0cAW4GNrb0RuH6s//x2FdLJwKNjh5kkSRPQ630KSQ4FTgXeM9b9IeCaJBcC9wPntP4bgDOAHYyuVLqgz9okSc/WayhU1Y+A1+7V931GVyPtPbaAi/qsR5L0/PbrO5q1/Jzy0VOGLmHRvvG+bwxdgrRkfCCeJKljKEiSOoaCJKljKEiSOoaCJKljKEiSOoaCJKljKEiSOoaCJKljKEiSOoaCJKljKEiSOj4QT5qgr77lF4YuYdF+4WtfHboETZAzBUlSx1CQJHUMBUlSx1CQJHUMBUlSx1CQJHUMBUlSp9dQSLIyybVJvpPk7iRvTnJEkhuT3NNeD29jk+TSJDuS3J7khD5rkyQ9W98zhUuAL1TVG4A3AncDFwPbqmodsK2tA5wOrGvLJuCynmuTJO2lt1BIchjwFuAKgKr6cVU9AmwAtrRhW4CzWnsDcFWN3ASsTHJUX/VJkp6tz5nCMcAscGWSW5NcnuRQYFVV7W5jHgRWtfZq4IGx/Xe2vmdIsinJTJKZ2dnZHsuXpANPn6GwAjgBuKyq3gT8iKcPFQFQVQXUYt60qjZX1XRVTU9NTS1ZsZKkfkNhJ7Czqm5u69cyComH5g4Ltdc9bfsu4Oix/de0PknShPQWClX1IPBAkmNb13rgLmArsLH1bQSub+2twPntKqSTgUfHDjNJkiag70dnvw/4ZJJDgHuBCxgF0TVJLgTuB85pY28AzgB2AE+0sZKkCeo1FKrqNmB6nk3r5xlbwEV91iNJen7e0SxJ6hgKkqSOoSBJ6hgKkqSOoSBJ6hgKkqSOoSBJ6hgKkqSOoSBJ6hgKkqSOoSBJ6hgKkqSOoSBJ6hgKkqSOoSBJ6hgKkqSOoSBJ6hgKkqSOoSBJ6hgKkqSOoSBJ6hgKkqROr6GQ5L4kdyS5LclM6zsiyY1J7mmvh7f+JLk0yY4ktyc5oc/aJEnPNomZwt+vquOrarqtXwxsq6p1wLa2DnA6sK4tm4DLJlCbJGnMEIePNgBbWnsLcNZY/1U1chOwMslRA9QnSQesvkOhgC8m2Z5kU+tbVVW7W/tBYFVrrwYeGNt3Z+t7hiSbkswkmZmdne2rbkk6IK3o+f1/rqp2JfnrwI1JvjO+saoqSS3mDatqM7AZYHp6elH7SpKeX68zhara1V73ANcBJwEPzR0Waq972vBdwNFju69pfZKkCektFJIcmuTVc23gF4E7ga3AxjZsI3B9a28Fzm9XIZ0MPDp2mEmSNAF9Hj5aBVyXZO5zPlVVX0jyTeCaJBcC9wPntPE3AGcAO4AngAt6rE2SNI/eQqGq7gXeOE//94H18/QXcFFf9UiSXph3NEuSOoaCJKljKEiSOoaCJKljKEiSOoaCJKljKEiSOoaCJKljKEiSOoaCJKljKEiSOoaCJKljKEiSOoaCJKljKEiSOoaCJKljKEiSOoaCJKljKEiSOoaCJKljKEiSOr2HQpKDktya5PNt/ZgkNyfZkeSPkhzS+l/e1ne07Wv7rk2S9EyTmCm8H7h7bP13gY9U1c8ADwMXtv4LgYdb/0faOEnSBPUaCknWAL8EXN7WA7wNuLYN2QKc1dob2jpt+/o2XpI0IX3PFH4f+FXgp239tcAjVfVUW98JrG7t1cADAG37o238MyTZlGQmyczs7GyPpUvSgae3UEjydmBPVW1fyvetqs1VNV1V01NTU0v51pJ0wFtQKCTZtpC+vZwCnJnkPuBqRoeNLgFWJlnRxqwBdrX2LuDo9t4rgMOA7y+kPknS0njeUEjyiiRHAEcmOTzJEW1Zy9OHfeZVVb9WVWuqai1wLvClqvonwJeBs9uwjcD1rb21rdO2f6mq6sV8KUnSi7PiBba/B/gA8DpgOzB34vcx4GMv8jP/LXB1kt8GbgWuaP1XAJ9IsgP4AaMgkSRN0POGQlVdAlyS5H1V9dEX+yFV9RXgK619L3DSPGOeBH75xX6GJOmle6GZAgBV9dEkPwusHd+nqq7qqS5J0gAWFApJPgH8LeA24CetuwBDQZKWkQWFAjANHOeJX0la3hZ6n8KdwN/osxBJ0vAWOlM4ErgryZ8BfzXXWVVn9lKVJGkQCw2F3+yzCEnSvmGhVx99te9CJEnDW+jVRz9kdLURwCHAwcCPquo1fRUmSZq8hc4UXj3Xbo+z3gCc3FdRkqRhLPopqTXyJ8A/XPpyJElDWujho3eOrb6M0X0LT/ZSkSRpMAu9+ugdY+2ngPsYHUKSJC0jCz2ncEHfhUiShrfQH9lZk+S6JHva8sft95clScvIQk80X8noR3Be15bPtT5J0jKy0FCYqqorq+qptnwc8AeSJWmZWWgofD/Ju5Mc1JZ34+8nS9Kys9BQ+GfAOcCDwG5Gv6H8T3uqSZI0kIVekvpbwMaqehggyRHAhxmFhSRpmVjoTOHvzgUCQFX9AHhTPyVJkoay0FB4WZLD51baTGGhswxJ0n5iof+w/x7wv5J8pq3/MvA7z7dDklcAXwNe3j7n2qr6jSTHAFcDrwW2A+dV1Y+TvJzRbz6fyOgk9j+uqvsW+X0kSS/BgmYKVXUV8E7goba8s6o+8QK7/RXwtqp6I3A8cFqSk4HfBT5SVT8DPAxc2MZfCDzc+j/SxkmSJmjBT0mtqruq6mNtuWsB46uqHm+rB7elgLcB17b+LcBZrb2hrdO2r2+P6ZYkTciiH529GO2ehtuAPcCNwHeBR6rqqTZkJ7C6tVcDDwC07Y8yOsQkSZqQXkOhqn5SVccDa4CTgDe81PdMsinJTJKZ2dnZl/p2kqQxvYbCnKp6BPgy8GZgZZK5E9xrgF2tvQs4GqBtP4x57pquqs1VNV1V01NTPmlDkpZSb6GQZCrJytZ+JXAqcDejcDi7DdsIXN/aW9s6bfuXqqqQJE1Mn/caHAVsSXIQo/C5pqo+n+Qu4Ookvw3cClzRxl8BfCLJDuAHwLk91iZJmkdvoVBVtzPPXc9VdS+j8wt79z/J6P4HSdJAJnJOQZK0fzAUJEkdQ0GS1DEUJEkdQ0GS1DEUJEkdQ0GS1DEUJEkdQ0GS1DEUJEkdQ0GS1DEUJEkdQ0GS1DEUJEkdQ0GS1DEUJEkdQ0GS1DEUJEkdQ0GS1DEUJEkdQ0GS1DEUJEmd3kIhydFJvpzkriTfTvL+1n9EkhuT3NNeD2/9SXJpkh1Jbk9yQl+1SZLm1+dM4SngV6rqOOBk4KIkxwEXA9uqah2wra0DnA6sa8sm4LIea5MkzaO3UKiq3VV1S2v/ELgbWA1sALa0YVuAs1p7A3BVjdwErExyVF/1SZKebcUkPiTJWuBNwM3Aqqra3TY9CKxq7dXAA2O77Wx9u8f6SLKJ0UyC17/+9f0VLWnRPvYrnxu6hEV57++9Y+gS9jm9n2hO8teAPwY+UFWPjW+rqgJqMe9XVZurarqqpqemppawUklSr6GQ5GBGgfDJqvps635o7rBQe93T+ncBR4/tvqb1SZImpM+rjwJcAdxdVf9pbNNWYGNrbwSuH+s/v12FdDLw6NhhJknSBPR5TuEU4DzgjiS3tb5/B3wIuCbJhcD9wDlt2w3AGcAO4Anggh5rkyTNo7dQqKqvA3mOzevnGV/ARX3VI0l6Yd7RLEnqGAqSpI6hIEnqGAqSpI6hIEnqGAqSpI6hIEnqGAqSpI6hIEnqGAqSpI6hIEnqGAqSpI6hIEnqGAqSpI6hIEnqGAqSpI6hIEnqGAqSpI6hIEnqGAqSpI6hIEnq9BYKSf4gyZ4kd471HZHkxiT3tNfDW3+SXJpkR5Lbk5zQV12SpOfW50zh48Bpe/VdDGyrqnXAtrYOcDqwri2bgMt6rEuS9Bx6C4Wq+hrwg726NwBbWnsLcNZY/1U1chOwMslRfdUmSZrfpM8prKqq3a39ILCqtVcDD4yN29n6niXJpiQzSWZmZ2f7q1SSDkCDnWiuqgLqRey3uaqmq2p6amqqh8ok6cA16VB4aO6wUHvd0/p3AUePjVvT+iRJEzTpUNgKbGztjcD1Y/3nt6uQTgYeHTvMJEmakBV9vXGSTwNvBY5MshP4DeBDwDVJLgTuB85pw28AzgB2AE8AF/RVlyTpufUWClX1rufYtH6esQVc1FctkqSF8Y5mSVLHUJAkdQwFSVLHUJAkdQwFSVLHUJAkdQwFSVLHUJAkdQwFSVKntzua1Y/v/dbfGbqERXv9v79j6BIkLZAzBUlSx1CQJHUMBUlSx1CQJHUMBUlSx1CQJHUMBUlSx1CQJHUMBUlSxzuaJWmBfufdZw9dwqL8+h9eu+h9nClIkjqGgiSps08dPkpyGnAJcBBweVV96MW8z4n/5qolratv2//j+UOXIEnAPjRTSHIQ8J+B04HjgHclOW7YqiTpwLLPhAJwErCjqu6tqh8DVwMbBq5Jkg4oqaqhawAgydnAaVX1z9v6ecDfq6r37jVuE7CprR4L/PkEyzwS+MsJft6k+f32X8v5u4Hfb6n9zaqamm/DPnVOYSGqajOweYjPTjJTVdNDfPYk+P32X8v5u4Hfb5L2pcNHu4Cjx9bXtD5J0oTsS6HwTWBdkmOSHAKcC2wduCZJOqDsM4ePquqpJO8F/gejS1L/oKq+PXBZexvksNUE+f32X8v5u4Hfb2L2mRPNkqTh7UuHjyRJAzMUJEkdQ2EBkrwiyZ8l+VaSbyf5D0PXtNSSHJTk1iSfH7qWpZbkviR3JLktyczQ9Sy1JCuTXJvkO0nuTvLmoWtaKkmObX9uc8tjST4wdF1LJckH278pdyb5dJJXDF6T5xReWJIAh1bV40kOBr4OvL+qbhq4tCWT5F8B08BrqurtQ9ezlJLcB0xX1bK8+SnJFuBPq+ryduXeq6rqkYHLWnLtUTi7GN3Uev/Q9bxUSVYz+rfkuKr6v0muAW6oqo8PWZczhQWokcfb6sFtWTZpmmQN8EvA5UPXosVJchjwFuAKgKr68XIMhGY98N3lEAhjVgCvTLICeBXwfwaux1BYqHZ45TZgD3BjVd08cElL6feBXwV+OnAdfSngi0m2t8ekLCfHALPAle3w3+VJDh26qJ6cC3x66CKWSlXtAj4MfA/YDTxaVV8ctipDYcGq6idVdTyjO61PSvK3By5pSSR5O7CnqrYPXUuPfq6qTmD0BN6Lkrxl6IKW0ArgBOCyqnoT8CPg4mFLWnrtsNiZwGeGrmWpJDmc0UM/jwFeBxya5N3DVmUoLFqbmn8ZOG3gUpbKKcCZ7bj71cDbkvzhsCUtrfY/MqpqD3AdoyfyLhc7gZ1jM9drGYXEcnM6cEtVPTR0IUvoHwB/UVWzVfX/gM8CPztwTYbCQiSZSrKytV8JnAp8Z9CilkhV/VpVramqtYym51+qqsH/t7JUkhya5NVzbeAXgTuHrWrpVNWDwANJjm1d64G7BiypL+9iGR06ar4HnJzkVe1ilvXA3QPXtO885mIfdxSwpV398DLgmqpadpduLlOrgOtGf+dYAXyqqr4wbElL7n3AJ9shlnuBCwauZ0m1MD8VeM/QtSylqro5ybXALcBTwK3sA4+78JJUSVLHw0eSpI6hIEnqGAqSpI6hIEnqGAqSpI6hIPUoydokd7b2dJJLW/utSQa/UUnam/cpSBNSVTPA3KO73wo8DvzPwQqS5uFMQXoOSX49yf9O8vX2rPt/neQrSabb9iPb40HmZgR/muSWtjxrFtBmB59Pshb4F8AH228E/HySv2iPZSfJa8bXpUlypiDNI8mJjB77cTyjvye3AM/30MA9wKlV9WSSdYweyTA938Cqui/JfwUer6oPt8/7CqPHl/9J+9zPtufhSBPlTEGa388D11XVE1X1GLD1BcYfDPy3JHcwepLncYv8vMt5+vEUFwBXLnJ/aUk4U5AW5yme/s/U+E8nfhB4CHhj2/7kYt60qr7RDkG9FTioqpbNQ/u0f3GmIM3va8BZSV7ZnrL6jtZ/H3Bia589Nv4wYHdV/RQ4DzjoBd7/h8Cr9+q7CvgUzhI0IENBmkdV3QL8EfAt4L8D32ybPgz8yyS3AkeO7fJfgI1JvgW8gdGP3TyfzwH/aO5Ec+v7JHA4y+8R0dqP+JRUaQGS/CZjJ4Z7+oyzgQ1VdV5fnyG9EM8pSPuAJB9l9OtiZwxdiw5szhQkSR3PKUiSOoaCJKljKEiSOoaCJKljKEiSOv8fFtBVpg+tnMAAAAAASUVORK5CYII=\n",
      "text/plain": "<Figure size 432x288 with 1 Axes>"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x = 'quality', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fixed acidity</th>\n      <th>volatile acidity</th>\n      <th>citric acid</th>\n      <th>residual sugar</th>\n      <th>chlorides</th>\n      <th>free sulfur dioxide</th>\n      <th>total sulfur dioxide</th>\n      <th>density</th>\n      <th>pH</th>\n      <th>sulphates</th>\n      <th>alcohol</th>\n      <th>quality</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7.4</td>\n      <td>0.700</td>\n      <td>0.00</td>\n      <td>1.9</td>\n      <td>0.076</td>\n      <td>11.0</td>\n      <td>34.0</td>\n      <td>0.99780</td>\n      <td>3.51</td>\n      <td>0.56</td>\n      <td>9.4</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7.8</td>\n      <td>0.880</td>\n      <td>0.00</td>\n      <td>2.6</td>\n      <td>0.098</td>\n      <td>25.0</td>\n      <td>67.0</td>\n      <td>0.99680</td>\n      <td>3.20</td>\n      <td>0.68</td>\n      <td>9.8</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7.8</td>\n      <td>0.760</td>\n      <td>0.04</td>\n      <td>2.3</td>\n      <td>0.092</td>\n      <td>15.0</td>\n      <td>54.0</td>\n      <td>0.99700</td>\n      <td>3.26</td>\n      <td>0.65</td>\n      <td>9.8</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11.2</td>\n      <td>0.280</td>\n      <td>0.56</td>\n      <td>1.9</td>\n      <td>0.075</td>\n      <td>17.0</td>\n      <td>60.0</td>\n      <td>0.99800</td>\n      <td>3.16</td>\n      <td>0.58</td>\n      <td>9.8</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7.4</td>\n      <td>0.700</td>\n      <td>0.00</td>\n      <td>1.9</td>\n      <td>0.076</td>\n      <td>11.0</td>\n      <td>34.0</td>\n      <td>0.99780</td>\n      <td>3.51</td>\n      <td>0.56</td>\n      <td>9.4</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1594</th>\n      <td>6.2</td>\n      <td>0.600</td>\n      <td>0.08</td>\n      <td>2.0</td>\n      <td>0.090</td>\n      <td>32.0</td>\n      <td>44.0</td>\n      <td>0.99490</td>\n      <td>3.45</td>\n      <td>0.58</td>\n      <td>10.5</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1595</th>\n      <td>5.9</td>\n      <td>0.550</td>\n      <td>0.10</td>\n      <td>2.2</td>\n      <td>0.062</td>\n      <td>39.0</td>\n      <td>51.0</td>\n      <td>0.99512</td>\n      <td>3.52</td>\n      <td>0.76</td>\n      <td>11.2</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1596</th>\n      <td>6.3</td>\n      <td>0.510</td>\n      <td>0.13</td>\n      <td>2.3</td>\n      <td>0.076</td>\n      <td>29.0</td>\n      <td>40.0</td>\n      <td>0.99574</td>\n      <td>3.42</td>\n      <td>0.75</td>\n      <td>11.0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1597</th>\n      <td>5.9</td>\n      <td>0.645</td>\n      <td>0.12</td>\n      <td>2.0</td>\n      <td>0.075</td>\n      <td>32.0</td>\n      <td>44.0</td>\n      <td>0.99547</td>\n      <td>3.57</td>\n      <td>0.71</td>\n      <td>10.2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1598</th>\n      <td>6.0</td>\n      <td>0.310</td>\n      <td>0.47</td>\n      <td>3.6</td>\n      <td>0.067</td>\n      <td>18.0</td>\n      <td>42.0</td>\n      <td>0.99549</td>\n      <td>3.39</td>\n      <td>0.66</td>\n      <td>11.0</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>1599 rows Ã— 12 columns</p>\n</div>",
      "text/plain": "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n0               7.4             0.700         0.00             1.9      0.076   \n1               7.8             0.880         0.00             2.6      0.098   \n2               7.8             0.760         0.04             2.3      0.092   \n3              11.2             0.280         0.56             1.9      0.075   \n4               7.4             0.700         0.00             1.9      0.076   \n...             ...               ...          ...             ...        ...   \n1594            6.2             0.600         0.08             2.0      0.090   \n1595            5.9             0.550         0.10             2.2      0.062   \n1596            6.3             0.510         0.13             2.3      0.076   \n1597            5.9             0.645         0.12             2.0      0.075   \n1598            6.0             0.310         0.47             3.6      0.067   \n\n      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n0                    11.0                  34.0  0.99780  3.51       0.56   \n1                    25.0                  67.0  0.99680  3.20       0.68   \n2                    15.0                  54.0  0.99700  3.26       0.65   \n3                    17.0                  60.0  0.99800  3.16       0.58   \n4                    11.0                  34.0  0.99780  3.51       0.56   \n...                   ...                   ...      ...   ...        ...   \n1594                 32.0                  44.0  0.99490  3.45       0.58   \n1595                 39.0                  51.0  0.99512  3.52       0.76   \n1596                 29.0                  40.0  0.99574  3.42       0.75   \n1597                 32.0                  44.0  0.99547  3.57       0.71   \n1598                 18.0                  42.0  0.99549  3.39       0.66   \n\n      alcohol  quality  \n0         9.4        2  \n1         9.8        2  \n2         9.8        2  \n3         9.8        3  \n4         9.4        2  \n...       ...      ...  \n1594     10.5        2  \n1595     11.2        3  \n1596     11.0        3  \n1597     10.2        2  \n1598     11.0        3  \n\n[1599 rows x 12 columns]"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class2idx = {\n",
    "    3:0,\n",
    "    4:1,\n",
    "    5:2,\n",
    "    6:3,\n",
    "    7:4,\n",
    "    8:5\n",
    "}\n",
    "\n",
    "idx2class = {v: k for k, v in class2idx.items()}\n",
    "\n",
    "df['quality'].replace(class2idx, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierDataset(Dataset):\n",
    "    '''\n",
    "    to set data in a block.\n",
    "    this dataset will be used by the dataloader to pass the data\n",
    "    into the model.\n",
    "    X = float\n",
    "    y = long\n",
    "    '''\n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 0:-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "# Split into train+val and test\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=69)\n",
    "\n",
    "# Split train into train-val\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.1, stratify=y_trainval, random_state=21)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_val, y_val = np.array(X_val), np.array(y_val)\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "\n",
    "\n",
    "train_dataset = ClassifierDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).long())\n",
    "val_dataset = ClassifierDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).long())\n",
    "test_dataset = ClassifierDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_distribution(obj):\n",
    "    count_dict = {\n",
    "        \"rating_3\": 0,\n",
    "        \"rating_4\": 0,\n",
    "        \"rating_5\": 0,\n",
    "        \"rating_6\": 0,\n",
    "        \"rating_7\": 0,\n",
    "        \"rating_8\": 0,\n",
    "    }\n",
    "    \n",
    "    for i in obj:\n",
    "        if i == 0: \n",
    "            count_dict['rating_3'] += 1\n",
    "        elif i == 1: \n",
    "            count_dict['rating_4'] += 1\n",
    "        elif i == 2: \n",
    "            count_dict['rating_5'] += 1\n",
    "        elif i == 3: \n",
    "            count_dict['rating_6'] += 1\n",
    "        elif i == 4: \n",
    "            count_dict['rating_7'] += 1  \n",
    "        elif i == 5: \n",
    "            count_dict['rating_8'] += 1              \n",
    "        else:\n",
    "            print(\"Check classes.\")\n",
    "            \n",
    "    return count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list = []\n",
    "for _, t in train_dataset:\n",
    "    target_list.append(t)\n",
    "    \n",
    "target_list = torch.tensor(target_list)\n",
    "target_list = target_list[torch.randperm(len(target_list))] \n",
    "\n",
    "class_count = [i for i in get_class_distribution(y_train).values()]\n",
    "class_weights = 1./torch.tensor(class_count, dtype=torch.float) \n",
    "\n",
    "class_weights_all = class_weights[target_list]\n",
    "\n",
    "weighted_sampler = WeightedRandomSampler(\n",
    "    weights=class_weights_all,\n",
    "    num_samples=len(class_weights_all),\n",
    "    replacement=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE= 16\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          sampler=weighted_sampler\n",
    ")\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=8)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch._C import device \n",
    "# import torch\n",
    "# import torch.optim as optim\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model = nn.Sequential(\n",
    "#     nn.Linear(11,4),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(4, 4),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(4, 6)\n",
    "# )\n",
    "# model = model.to(device)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# LOSS = []\n",
    "# from matplotlib import pyplot as plt\n",
    "# for i in range(100):\n",
    "#     loss = 0\n",
    "#     for x, y in train_loader:\n",
    "#         x, y = x.to(device), y.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         y_pred = model(x)\n",
    "#         train_loss = criterion(y_pred, y)\n",
    "\n",
    "#         train_loss.backward()\n",
    "#         optimizer.step()\n",
    "#         loss += train_loss \n",
    "#     LOSS.append(loss/len(train_loader))\n",
    "\n",
    "# plt.plot(LOSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sai/.local/lib/python3.6/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:115.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin_training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 18.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin_training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 17.09it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch._C import device \n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from pyt_ex import neural_net as neural_network \n",
    "import yaml\n",
    "\n",
    "with open('config/Input.yaml') as File:\n",
    "        dic = yaml.load(File, Loader=yaml.FullLoader)\n",
    "\n",
    "for i in dic:\n",
    "    ann = neural_network.Ann(dic[i])\n",
    "    ann = ann.to(device)\n",
    "#     ann.show()\n",
    "    ann.train_val(train_dataloader=train_loader, val_dataloader=val_loader)\n",
    "    # graphics = neural_network.grahics()\n",
    "    # ann.loss_plot()\n",
    "    # ann.accuracy_plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "name": "python369jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  },
  "orig_nbformat": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}