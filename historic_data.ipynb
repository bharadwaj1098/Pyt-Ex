{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader,WeightedRandomSampler\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler    \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "Input_Matrix = pd.read_excel('Data/as of 2020 Historical Data.xlsx', sheet_name='Input Matrix')\n",
    "\n",
    "Target_Matrix = pd.read_excel('/home/sai/Pyt-Ex/Data/as of 2020 Historical Data.xlsx', sheet_name='Target Matrix')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Unnamed: 1</th>\n      <th>Inputs (at Landfall)</th>\n      <th>Unnamed: 3</th>\n      <th>Unnamed: 4</th>\n      <th>Unnamed: 5</th>\n      <th>Unnamed: 6</th>\n      <th>First Landfall Location</th>\n      <th>Unnamed: 8</th>\n      <th>Second Landfall Location</th>\n      <th>Unnamed: 10</th>\n      <th>Third Landfall Location</th>\n      <th>Unnamed: 12</th>\n      <th>Fourth Landfall Location</th>\n      <th>Unnamed: 14</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Year</td>\n      <td>Storm</td>\n      <td>Population Affected</td>\n      <td>Pressure (mbar)</td>\n      <td>Wind Speed (mph)</td>\n      <td>Storm Surge (ft)</td>\n      <td>Precip (inches)</td>\n      <td>Latitude</td>\n      <td>Longtitude</td>\n      <td>Latitude</td>\n      <td>Longtitude</td>\n      <td>Latitude</td>\n      <td>Longtitude</td>\n      <td>Latitude</td>\n      <td>Longtitude</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2020</td>\n      <td>Bertha</td>\n      <td>710000</td>\n      <td>1005</td>\n      <td>51.75</td>\n      <td>1.32</td>\n      <td>15</td>\n      <td>32.9</td>\n      <td>79.7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>NaN</td>\n      <td>Cristobal</td>\n      <td>7200000</td>\n      <td>990</td>\n      <td>51.75</td>\n      <td>6.2</td>\n      <td>13.65</td>\n      <td>29.3</td>\n      <td>89.8</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>NaN</td>\n      <td>Fay</td>\n      <td>22500000</td>\n      <td>999</td>\n      <td>51.75</td>\n      <td>2.67</td>\n      <td>6.97</td>\n      <td>39.4</td>\n      <td>74.4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NaN</td>\n      <td>Hanna</td>\n      <td>2000000</td>\n      <td>973</td>\n      <td>92</td>\n      <td>6.24</td>\n      <td>15.49</td>\n      <td>26.8</td>\n      <td>97.3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>92</th>\n      <td>NaN</td>\n      <td>H. Earl</td>\n      <td>1600000</td>\n      <td>987</td>\n      <td>80.5</td>\n      <td>5.3</td>\n      <td>16.38</td>\n      <td>30.1</td>\n      <td>85.7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>NaN</td>\n      <td>TS. Frances</td>\n      <td>17000000</td>\n      <td>990</td>\n      <td>51.75</td>\n      <td>5.1</td>\n      <td>11.38</td>\n      <td>28.2</td>\n      <td>96.9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>94</th>\n      <td>NaN</td>\n      <td>H. Georges</td>\n      <td>11200000</td>\n      <td>964</td>\n      <td>103.5</td>\n      <td>9</td>\n      <td>38.46</td>\n      <td>24.5</td>\n      <td>81.8</td>\n      <td>30.4</td>\n      <td>88.9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>NaN</td>\n      <td>TS. Hermine</td>\n      <td>2200000</td>\n      <td>1000</td>\n      <td>40.25</td>\n      <td>0</td>\n      <td>1</td>\n      <td>29.1</td>\n      <td>90.9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>NaN</td>\n      <td>H. Mitch</td>\n      <td>7400000</td>\n      <td>989</td>\n      <td>63.25</td>\n      <td>3</td>\n      <td>7</td>\n      <td>26.2</td>\n      <td>81.9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>97 rows Ã— 15 columns</p>\n</div>",
      "text/plain": "   Unnamed: 0   Unnamed: 1 Inputs (at Landfall)       Unnamed: 3  \\\n0        Year        Storm  Population Affected  Pressure (mbar)   \n1        2020       Bertha               710000             1005   \n2         NaN    Cristobal              7200000              990   \n3         NaN          Fay             22500000              999   \n4         NaN        Hanna              2000000              973   \n..        ...          ...                  ...              ...   \n92        NaN      H. Earl              1600000              987   \n93        NaN  TS. Frances             17000000              990   \n94        NaN   H. Georges             11200000              964   \n95        NaN  TS. Hermine              2200000             1000   \n96        NaN     H. Mitch              7400000              989   \n\n          Unnamed: 4        Unnamed: 5       Unnamed: 6  \\\n0   Wind Speed (mph)  Storm Surge (ft)  Precip (inches)   \n1              51.75              1.32               15   \n2              51.75               6.2            13.65   \n3              51.75              2.67             6.97   \n4                 92              6.24            15.49   \n..               ...               ...              ...   \n92              80.5               5.3            16.38   \n93             51.75               5.1            11.38   \n94             103.5                 9            38.46   \n95             40.25                 0                1   \n96             63.25                 3                7   \n\n   First Landfall Location  Unnamed: 8 Second Landfall Location Unnamed: 10  \\\n0                 Latitude  Longtitude                 Latitude  Longtitude   \n1                     32.9        79.7                        0           0   \n2                     29.3        89.8                        0           0   \n3                     39.4        74.4                        0           0   \n4                     26.8        97.3                        0           0   \n..                     ...         ...                      ...         ...   \n92                    30.1        85.7                        0           0   \n93                    28.2        96.9                        0           0   \n94                    24.5        81.8                     30.4        88.9   \n95                    29.1        90.9                        0           0   \n96                    26.2        81.9                        0           0   \n\n   Third Landfall Location Unnamed: 12 Fourth Landfall Location Unnamed: 14  \n0                 Latitude  Longtitude                 Latitude  Longtitude  \n1                        0           0                        0           0  \n2                        0           0                        0           0  \n3                        0           0                        0           0  \n4                        0           0                        0           0  \n..                     ...         ...                      ...         ...  \n92                       0           0                        0           0  \n93                       0           0                        0           0  \n94                       0           0                        0           0  \n95                       0           0                        0           0  \n96                       0           0                        0           0  \n\n[97 rows x 15 columns]"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Input_Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Year</th>\n      <th>Storm</th>\n      <th>Ry_Damage</th>\n      <th>2016_USD</th>\n      <th>Impact_Level</th>\n      <th>IL0</th>\n      <th>IL1</th>\n      <th>IL2</th>\n      <th>IL3</th>\n      <th>IL4</th>\n      <th>IL5</th>\n      <th>Check</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2020</td>\n      <td>Bertha</td>\n      <td>NaN</td>\n      <td>0.000000e+00</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>Cristobal</td>\n      <td>3.100000e+08</td>\n      <td>2.666000e+08</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>NaN</td>\n      <td>Fay</td>\n      <td>2.200000e+08</td>\n      <td>1.892000e+08</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>NaN</td>\n      <td>Hanna</td>\n      <td>1.100000e+09</td>\n      <td>9.460000e+08</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NaN</td>\n      <td>Isaias</td>\n      <td>4.800000e+09</td>\n      <td>4.128000e+09</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>91</th>\n      <td>NaN</td>\n      <td>H. Earl</td>\n      <td>7.900000e+07</td>\n      <td>1.374509e+08</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>92</th>\n      <td>NaN</td>\n      <td>TS. Frances</td>\n      <td>5.000000e+08</td>\n      <td>8.699422e+08</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>NaN</td>\n      <td>H. Georges</td>\n      <td>5.900000e+09</td>\n      <td>1.026532e+10</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>94</th>\n      <td>NaN</td>\n      <td>TS. Hermine</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>NaN</td>\n      <td>H. Mitch</td>\n      <td>4.000000e+07</td>\n      <td>6.959538e+07</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>96 rows Ã— 12 columns</p>\n</div>",
      "text/plain": "    Year        Storm     Ry_Damage      2016_USD  Impact_Level  IL0  IL1  \\\n0   2020       Bertha           NaN  0.000000e+00             0    1    0   \n1    NaN    Cristobal  3.100000e+08  2.666000e+08             2    0    0   \n2    NaN          Fay  2.200000e+08  1.892000e+08             2    0    0   \n3    NaN        Hanna  1.100000e+09  9.460000e+08             2    0    0   \n4    NaN       Isaias  4.800000e+09  4.128000e+09             3    0    0   \n..   ...          ...           ...           ...           ...  ...  ...   \n91   NaN      H. Earl  7.900000e+07  1.374509e+08             2    0    0   \n92   NaN  TS. Frances  5.000000e+08  8.699422e+08             2    0    0   \n93   NaN   H. Georges  5.900000e+09  1.026532e+10             4    0    0   \n94   NaN  TS. Hermine  0.000000e+00  0.000000e+00             0    1    0   \n95   NaN     H. Mitch  4.000000e+07  6.959538e+07             1    0    1   \n\n    IL2  IL3  IL4  IL5  Check  \n0     0    0    0    0      0  \n1     1    0    0    0      2  \n2     1    0    0    0      2  \n3     1    0    0    0      2  \n4     0    1    0    0      3  \n..  ...  ...  ...  ...    ...  \n91    1    0    0    0      2  \n92    1    0    0    0      2  \n93    0    0    1    0      4  \n94    0    0    0    0      0  \n95    0    0    0    0      1  \n\n[96 rows x 12 columns]"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_columns = ['Year', 'Storm', 'Ry_Damage', '2016_USD', 'Impact_Level', 'IL0', 'IL1', 'IL2', 'IL3', 'IL4', 'IL5', 'Check']\n",
    "\n",
    "Target_Matrix.drop(['Unnamed: 11', 'Unnamed: 13'], axis=1, inplace=True)\n",
    "\n",
    "Target_Matrix.set_axis(target_columns, axis=1, inplace=True)\n",
    "\n",
    "Target_Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Year            72\nStorm            0\nRy_Damage        1\n2016_USD         1\nImpact_Level     0\nIL0              0\nIL1              0\nIL2              0\nIL3              0\nIL4              0\nIL5              0\nCheck            0\ndtype: int64"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Target_Matrix.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Population_Affected</th>\n      <th>Pressure(mbar)</th>\n      <th>Wind_Speed(mph)</th>\n      <th>Storm_Surge(ft)</th>\n      <th>Precip(inches)</th>\n      <th>First_Latitude</th>\n      <th>First_Longtitude</th>\n      <th>Second_Latitude</th>\n      <th>Second_Longtitude</th>\n      <th>Third_Latitude</th>\n      <th>Third_Longtitude</th>\n      <th>Fourth_Latitude</th>\n      <th>Fourth_Longtitude</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>710000</td>\n      <td>1005</td>\n      <td>51.75</td>\n      <td>1.32</td>\n      <td>15</td>\n      <td>32.9</td>\n      <td>79.7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7200000</td>\n      <td>990</td>\n      <td>51.75</td>\n      <td>6.2</td>\n      <td>13.65</td>\n      <td>29.3</td>\n      <td>89.8</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>22500000</td>\n      <td>999</td>\n      <td>51.75</td>\n      <td>2.67</td>\n      <td>6.97</td>\n      <td>39.4</td>\n      <td>74.4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2000000</td>\n      <td>973</td>\n      <td>92</td>\n      <td>6.24</td>\n      <td>15.49</td>\n      <td>26.8</td>\n      <td>97.3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>56300000</td>\n      <td>986</td>\n      <td>92</td>\n      <td>6.3</td>\n      <td>8.85</td>\n      <td>33.9</td>\n      <td>78.5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>91</th>\n      <td>1600000</td>\n      <td>987</td>\n      <td>80.5</td>\n      <td>5.3</td>\n      <td>16.38</td>\n      <td>30.1</td>\n      <td>85.7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>92</th>\n      <td>17000000</td>\n      <td>990</td>\n      <td>51.75</td>\n      <td>5.1</td>\n      <td>11.38</td>\n      <td>28.2</td>\n      <td>96.9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>11200000</td>\n      <td>964</td>\n      <td>103.5</td>\n      <td>9</td>\n      <td>38.46</td>\n      <td>24.5</td>\n      <td>81.8</td>\n      <td>30.4</td>\n      <td>88.9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>94</th>\n      <td>2200000</td>\n      <td>1000</td>\n      <td>40.25</td>\n      <td>0</td>\n      <td>1</td>\n      <td>29.1</td>\n      <td>90.9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>7400000</td>\n      <td>989</td>\n      <td>63.25</td>\n      <td>3</td>\n      <td>7</td>\n      <td>26.2</td>\n      <td>81.9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>96 rows Ã— 13 columns</p>\n</div>",
      "text/plain": "   Population_Affected Pressure(mbar) Wind_Speed(mph) Storm_Surge(ft)  \\\n0               710000           1005           51.75            1.32   \n1              7200000            990           51.75             6.2   \n2             22500000            999           51.75            2.67   \n3              2000000            973              92            6.24   \n4             56300000            986              92             6.3   \n..                 ...            ...             ...             ...   \n91             1600000            987            80.5             5.3   \n92            17000000            990           51.75             5.1   \n93            11200000            964           103.5               9   \n94             2200000           1000           40.25               0   \n95             7400000            989           63.25               3   \n\n   Precip(inches) First_Latitude First_Longtitude Second_Latitude  \\\n0              15           32.9             79.7               0   \n1           13.65           29.3             89.8               0   \n2            6.97           39.4             74.4               0   \n3           15.49           26.8             97.3               0   \n4            8.85           33.9             78.5               0   \n..            ...            ...              ...             ...   \n91          16.38           30.1             85.7               0   \n92          11.38           28.2             96.9               0   \n93          38.46           24.5             81.8            30.4   \n94              1           29.1             90.9               0   \n95              7           26.2             81.9               0   \n\n   Second_Longtitude Third_Latitude Third_Longtitude Fourth_Latitude  \\\n0                  0              0                0               0   \n1                  0              0                0               0   \n2                  0              0                0               0   \n3                  0              0                0               0   \n4                  0              0                0               0   \n..               ...            ...              ...             ...   \n91                 0              0                0               0   \n92                 0              0                0               0   \n93              88.9              0                0               0   \n94                 0              0                0               0   \n95                 0              0                0               0   \n\n   Fourth_Longtitude  \n0                  0  \n1                  0  \n2                  0  \n3                  0  \n4                  0  \n..               ...  \n91                 0  \n92                 0  \n93                 0  \n94                 0  \n95                 0  \n\n[96 rows x 13 columns]"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_columns = ['Year', 'Storm', 'Population_Affected', 'Pressure(mbar)', 'Wind_Speed(mph)', 'Storm_Surge(ft)', 'Precip(inches)', 'First_Latitude', 'First_Longtitude', 'Second_Latitude', 'Second_Longtitude', 'Third_Latitude', 'Third_Longtitude', 'Fourth_Latitude', 'Fourth_Longtitude']\n",
    "\n",
    "Input_Matrix.set_axis(input_columns, axis=1, inplace=True)\n",
    "\n",
    "Input_Matrix.drop([0], inplace = True)\n",
    "\n",
    "Input_Matrix.reset_index(drop=True, inplace=True)\n",
    "\n",
    "Input_Matrix.drop(['Year', 'Storm'], axis=1, inplace=True)\n",
    "\n",
    "Input_Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Population_Affected    0\nPressure(mbar)         0\nWind_Speed(mph)        0\nStorm_Surge(ft)        0\nPrecip(inches)         0\nFirst_Latitude         0\nFirst_Longtitude       0\nSecond_Latitude        0\nSecond_Longtitude      0\nThird_Latitude         0\nThird_Longtitude       0\nFourth_Latitude        0\nFourth_Longtitude      0\ndtype: int64"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Input_Matrix.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Input_Matrix\n",
    "df['Impact_Level'] = Target_Matrix['Impact_Level'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<AxesSubplot:xlabel='Impact_Level', ylabel='count'>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": "<Figure size 432x288 with 1 Axes>"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x = 'Impact_Level', data=df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierDataset(Dataset):\n",
    "    '''\n",
    "    to set data in a block.\n",
    "    this dataset will be used by the dataloader to pass the data\n",
    "    into the model.\n",
    "    X = float\n",
    "    y = long\n",
    "    '''\n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 0:-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "# Split into train+val and test\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=69)\n",
    "\n",
    "# Split train into train-val\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.1, stratify=y_trainval, random_state=21)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_val, y_val = np.array(X_val), np.array(y_val)\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "\n",
    "X = np.array(scaler.transform(X))\n",
    "y = np.array(y)\n",
    "\n",
    "train_dataset = ClassifierDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).long())\n",
    "val_dataset = ClassifierDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).long())\n",
    "test_dataset = ClassifierDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).long())\n",
    "dataset = ClassifierDataset(torch.from_numpy(X).float(), torch.from_numpy(y).long())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_distribution(obj):\n",
    "    count_dict = {\n",
    "        \"rating_3\": 0,\n",
    "        \"rating_4\": 0,\n",
    "        \"rating_5\": 0,\n",
    "        \"rating_6\": 0,\n",
    "        \"rating_7\": 0,\n",
    "        \"rating_8\": 0,\n",
    "    }\n",
    "    \n",
    "    for i in obj:\n",
    "        if i == 0: \n",
    "            count_dict['rating_3'] += 1\n",
    "        elif i == 1: \n",
    "            count_dict['rating_4'] += 1\n",
    "        elif i == 2: \n",
    "            count_dict['rating_5'] += 1\n",
    "        elif i == 3: \n",
    "            count_dict['rating_6'] += 1\n",
    "        elif i == 4: \n",
    "            count_dict['rating_7'] += 1  \n",
    "        elif i == 5: \n",
    "            count_dict['rating_8'] += 1              \n",
    "        else:\n",
    "            print(\"Check classes.\")\n",
    "            \n",
    "    return count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list = []\n",
    "for _, t in train_dataset:\n",
    "    target_list.append(t)\n",
    "    \n",
    "target_list = torch.tensor(target_list)\n",
    "target_list = target_list[torch.randperm(len(target_list))] \n",
    "\n",
    "class_count = [i for i in get_class_distribution(y_train).values()]\n",
    "class_weights = 1./torch.tensor(class_count, dtype=torch.float) \n",
    "\n",
    "class_weights_all = class_weights[target_list]\n",
    "\n",
    "weighted_sampler = WeightedRandomSampler(\n",
    "    weights=class_weights_all,\n",
    "    num_samples=len(class_weights_all),\n",
    "    replacement=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sai/.local/lib/python3.6/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:115.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from torch._C import device \n",
    "import torch\n",
    "# torch.manual_seed(10)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from pyt_ex import neural_net as neural_network \n",
    "import yaml\n",
    "\n",
    "with open('config/Input.yaml') as File:\n",
    "        dic = yaml.load(File, Loader=yaml.FullLoader)\n",
    "\n",
    "\n",
    "plots = {}\n",
    "for i in dic:\n",
    "    plots[i] = {}\n",
    "\n",
    "    batch_size = dic[i][\"batch_size\"]\n",
    "    train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          sampler=weighted_sampler\n",
    "                            )\n",
    "    val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size) \n",
    "    full_dataloader = DataLoader(dataset=dataset)\n",
    "\n",
    "    ann = neural_network.Ann(dic[i])\n",
    "    ann = ann.to(device)\n",
    "    ann.train_val(train_dataloader=train_loader, val_dataloader=val_loader, test_dataloader=test_loader)\n",
    "\n",
    "    plots[i][\"train_acc_list\"] = ann.train_acc_list\n",
    "    plots[i][\"val_acc_list\"] = ann.val_acc_list\n",
    "    plots[i][\"train_loss_list\"] = ann.train_loss_list\n",
    "    plots[i][\"val_loss_list\"] = ann.val_loss_list\n",
    "#     plots[i][\"Output\"] = ann.Output\n",
    "    plots[i][\"test_output\"] = ann.test_Output\n",
    "\n",
    "    plots[i][\"Predictions\"], plots[i]['Final_acc'] = ann.model_final(dataloader=full_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def model_plots(i):\n",
    "#     from matplotlib.gridspec import GridSpec\n",
    "#     fig = plt.figure(figsize=(20, 10))\n",
    "#     gs = GridSpec(nrows=2, ncols=2)\n",
    "\n",
    "#     ax0 = fig.add_subplot(gs[0, 0])\n",
    "#     ax0.plot(plots[i][\"train_acc_list\"], label = 'train_acc_list')\n",
    "#     ax0.plot(plots[i][\"val_acc_list\"], label = 'val_acc_list')\n",
    "#     ax0.legend()\n",
    "#     ax0.title.set_text('Training and validation accuracy')\n",
    "#     ax0.set(xlabel='Iterations', ylabel='Accuracy')\n",
    "\n",
    "#     ax1 = fig.add_subplot(gs[0, 1])\n",
    "#     ax1.plot(plots[i][\"train_loss_list\"], label = 'train_loss_list')\n",
    "#     ax1.plot(plots[i][\"val_loss_list\"], label = 'val_loss_list')\n",
    "#     ax1.legend() \n",
    "#     ax1.title.set_text('Training and validation loss')\n",
    "#     ax1.set(xlabel='Iterations', ylabel='CrossEntropyLoss')\n",
    "\n",
    "#     ax2 = fig.add_subplot(gs[1, 0])\n",
    "#     confusion_matrix_df = pd.DataFrame(confusion_matrix(y_test, plots[i][\"test_output\"]))\n",
    "#     ax2 = sns.heatmap(confusion_matrix_df, annot=True, cmap='Blues')\n",
    "#     ax2.title.set_text('Test Confusion matrix')\n",
    "#     ax2.set(xlabel='Predicted Label', ylabel='True Label')\n",
    "\n",
    "#     # ax3 = fig.add_subplot()\n",
    "#     ax3 = fig.add_subplot(gs[1, 1])\n",
    "#     confusion_matrix_df = pd.DataFrame(confusion_matrix(y, plots[i][\"Predictions\"]))\n",
    "#     ax3 = sns.heatmap(confusion_matrix_df, annot=True, cmap='Blues')\n",
    "#     ax3.title.set_text('Full Confusion matrix, final_acc = '+str( plots[i]['Final_acc']))\n",
    "#     ax3.set(xlabel='Predicted Label', ylabel='True Label')\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "# models = [i for i in plots.keys()]\n",
    "# models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(models[0])\n",
    "# model_plots(models[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_plots(models[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:21<00:00, 40.51s/it]\n"
     ]
    }
   ],
   "source": [
    "logger = {}\n",
    "seeds = [2, 3, 7, 11, 13, 17, 19, 23, 29, 31]\n",
    "\n",
    "with open('config/Input.yaml') as File:\n",
    "    dic = yaml.load(File, Loader=yaml.FullLoader)\n",
    "from tqdm import tqdm\n",
    "        \n",
    "for i in tqdm(dic):\n",
    "    logger[i] = {}\n",
    "\n",
    "    batch_size = dic[i][\"batch_size\"]\n",
    "    for j in seeds:\n",
    "        # print(i)\n",
    "        X = df.iloc[:, 0:-1]\n",
    "        y = df.iloc[:, -1]\n",
    "\n",
    "        # Split into train+val and test\n",
    "        X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=j)\n",
    "\n",
    "        # Split train into train-val\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.1, stratify=y_trainval, random_state=j)\n",
    "\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "        X_val, y_val = np.array(X_val), np.array(y_val)\n",
    "        X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "\n",
    "        X = np.array(scaler.transform(X))\n",
    "        y = np.array(y)\n",
    "\n",
    "        train_dataset = ClassifierDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).long())\n",
    "        val_dataset = ClassifierDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).long())\n",
    "        test_dataset = ClassifierDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).long())\n",
    "        dataset = ClassifierDataset(torch.from_numpy(X).float(), torch.from_numpy(y).long())\n",
    "\n",
    "        target_list = []\n",
    "        for _, t in train_dataset:\n",
    "            target_list.append(t)\n",
    "            \n",
    "        target_list = torch.tensor(target_list)\n",
    "        target_list = target_list[torch.randperm(len(target_list))] \n",
    "\n",
    "        class_count = [i for i in get_class_distribution(y_train).values()]\n",
    "        class_weights = 1./torch.tensor(class_count, dtype=torch.float) \n",
    "\n",
    "        class_weights_all = class_weights[target_list]\n",
    "\n",
    "        weighted_sampler = WeightedRandomSampler(\n",
    "            weights=class_weights_all,\n",
    "            num_samples=len(class_weights_all),\n",
    "            replacement=True\n",
    "        )\n",
    "\n",
    "        train_loader = DataLoader(dataset=train_dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            sampler=weighted_sampler\n",
    "                                )\n",
    "\n",
    "        val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size)\n",
    "        test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size) \n",
    "        full_dataloader = DataLoader(dataset=dataset)\n",
    "\n",
    "        ann = neural_network.Ann(dic[i])\n",
    "        ann = ann.to(device)\n",
    "        ann.train_val(train_dataloader=train_loader, val_dataloader=val_loader, test_dataloader=test_loader)\n",
    "        unique = [0, 1, 2, 3, 4, 5]\n",
    "        # if ann.test_acc >= 50:\n",
    "        logger[i][j] = {}\n",
    "        logger[i][j]['y_test'] = y_test\n",
    "        logger[i][j][\"train_acc_list\"] = ann.train_acc_list\n",
    "        logger[i][j][\"val_acc_list\"] = ann.val_acc_list\n",
    "        logger[i][j][\"train_loss_list\"] = ann.train_loss_list\n",
    "        logger[i][j][\"val_loss_list\"] = ann.val_loss_list\n",
    "        logger[i][j][\"test_output\"] = ann.test_Output\n",
    "        logger[i][j][\"test_acc\"] = ann.test_acc\n",
    "        logger[i][j][\"Predictions\"], logger[i][j]['Final_acc'] = ann.model_final(dataloader=full_dataloader)\n",
    "        logger[i][j]['classification_report'] = classification_report(y_test, ann.test_Output, labels = unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys([2, 3, 7, 11, 13, 17, 19, 23, 29, 31])"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logger['ONE'].keys() \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys([2, 3, 7, 11, 13, 17, 19, 23, 29, 31])"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger['TWO'].keys() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.57      0.67         7\n",
      "           1       0.33      1.00      0.50         1\n",
      "           2       0.50      0.60      0.55         5\n",
      "           3       0.75      0.75      0.75         4\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.65        20\n",
      "   macro avg       0.56      0.65      0.58        20\n",
      "weighted avg       0.67      0.65      0.64        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(logger['TWO'][13]['classification_report']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.0\n"
     ]
    }
   ],
   "source": [
    "print(logger['TWO'][13]['test_acc']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a = logger['TWO'][13]['test_output']\n",
    "b = logger['TWO'][13]['y_test']\n",
    "c = 0\n",
    "for i in range(len(b)):\n",
    "    if a[i] == b[i]:\n",
    "        c+=1\n",
    "print(c/len(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "name": "python369jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  },
  "orig_nbformat": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}